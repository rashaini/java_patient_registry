{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python2.7/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "import urllib\n",
    "from urlparse import urljoin\n",
    "import urlparse\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "mydict = {}\n",
    "map_url_text = {}\n",
    "tokens = []\n",
    "inverted = {}\n",
    "\n",
    "# store homepage url:text dict 'mydict'\n",
    "url = 'https://www.macmillan.org.uk'\n",
    "response = urllib.urlopen(url).read()\n",
    "soup1 = BeautifulSoup(response, \"lxml\")\n",
    "text = soup1.get_text()\n",
    "mydict[url] = text\n",
    "\n",
    "# extract links from homepage text and store them in list with unique elements 'links'\n",
    "soup2 = BeautifulSoup(response, parse_only=SoupStrainer('a', href=True))\n",
    "links = soup2.find_all('a')\n",
    "links = [s.get('href') for s in links]\n",
    "links = [unicode(s) for s in links]\n",
    "links = [s for s in links if (s.startswith(\"https://\") or s.startswith(\"http://\"))]\n",
    "# links = set(links)\n",
    "\n",
    "# fetch text for each url in 'links' and store them in dict 'map_url_text'\n",
    "for l in links:\n",
    "    r = urllib.urlopen(l).read()\n",
    "    soup = BeautifulSoup(r, \"lxml\")\n",
    "    for s in soup([\"script\", \"style\"]):\n",
    "        s.extract()\n",
    "    t = soup.get_text()\n",
    "    lines = (line.strip() for line in t.splitlines())\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    t = ' '.join(chunk for chunk in chunks if chunk)\n",
    "    map_url_text[l] = t\n",
    "\n",
    "# tokenize text in 'map_url_text' and add unique tokens to list 'tokens'\n",
    "for key, value in map_url_text.iteritems():\n",
    "    value = set(value.split())\n",
    "    for word in value:\n",
    "        tokens.append(word)\n",
    "\n",
    "# build inverted index\n",
    "# for k in tokens:\n",
    "#     for key, value in map_url_text.iteritems():\n",
    "#         if k in value:\n",
    "#             if any(k is term for term, urls in inverted.iteritems()):\n",
    "#                 if key not in inverted[k]:\n",
    "#                     inverted[k].append(key) \n",
    "#             else:\n",
    "#                 inverted[k] = [key]\n",
    "\n",
    "# print inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "from pprint import pprint \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# 'documents' are list of texts\n",
    "documents = (value for key, value in map_url_text.iteritems())\n",
    "\n",
    "# remove common words and tokenize\n",
    "stoplist = set(stopwords.words('english')) and ('@ + 0 1 2 3 4 5 6 7 8 9 / . , = - '.split()) \n",
    "texts = [[word for word in document.lower().split() if word not in stoplist] \n",
    "         for document in documents]\n",
    "\n",
    "# assign unique ids to unique tokens from texts\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "# dictionary.save('/example.dict')  # store the dictionary, for future reference\n",
    "\n",
    "# build inverted index from dictionary\n",
    "# for tok in dictionary:\n",
    "#     for key, value in map_url_text.iteritems():\n",
    "#         if dictionary[tok] in value:\n",
    "#             if any(dictionary[tok] is term for term, urls in inverted.iteritems()):\n",
    "#                 if key not in inverted[(dictionary[tok])]:\n",
    "#                     inverted[(dictionary[tok])].append(key) \n",
    "#             else:\n",
    "#                 inverted[(dictionary[tok])] = [key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build positional index\n",
    "for tok in dictionary:\n",
    "    for key, value in map_url_text.iteritems():\n",
    "        for word in value:\n",
    "            if dictionary[tok] is word:\n",
    "                d = {key:[value.index(word)]}\n",
    "                if any(dictionary[tok] is term for term, urls in inverted.iteritems()):\n",
    "                    if key not in inverted[(dictionary[tok])]:\n",
    "                        inverted[(dictionary[tok])].append(d) \n",
    "                else:\n",
    "                    inverted[(dictionary[tok])] = [d]\n",
    "                    \n",
    "print inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello doc1 0\n",
      "school doc2 4\n",
      "school doc3 8\n",
      "bye doc4 0\n",
      "London doc4 4\n",
      "uni doc3 0\n"
     ]
    }
   ],
   "source": [
    "positional = {}\n",
    "\n",
    "myDict = {}\n",
    "myDict['1'] = 'hello'\n",
    "myDict['2'] = 'bye'\n",
    "myDict['3'] = 'school'\n",
    "myDict['4'] = 'uni'\n",
    "myDict['5'] = 'London'\n",
    "\n",
    "mapUrlText = {}\n",
    "mapUrlText['doc1'] = 'hello this is london'\n",
    "mapUrlText['doc2'] = 'the school is nice'\n",
    "mapUrlText['doc3'] = 'uni and school'\n",
    "mapUrlText['doc4'] = 'bye London'\n",
    "\n",
    "# for tok, item in myDict.iteritems():\n",
    "#     for key, value in mapUrlText.iteritems():\n",
    "#         for word in value:\n",
    "#             if item == word:\n",
    "#                 d = {key:[value.index(word)]}\n",
    "#                 if any(item is term for term, urls in positional.iteritems()):\n",
    "#                     if key not in positional[item]:\n",
    "#                         positional[item].append(d)\n",
    "#                 else:\n",
    "#                     positional[item] = [d]\n",
    "\n",
    "#  for tok, item in myDict.iteritems():\n",
    "#     for key, value in mapUrlText.iteritems():\n",
    "#         for word in value.split():\n",
    "#             if item is word:\n",
    "#                 print key, value.index(word)\n",
    "\n",
    "for tok, item in myDict.iteritems():\n",
    "    for key, value in mapUrlText.iteritems():\n",
    "#         print len(value.split())\n",
    "        for word in value.split():\n",
    "            if item == word:\n",
    "                print word, key, value.index(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from decimal import *\n",
    "import operator\n",
    "\n",
    "# map tokens<->number of docs in which they appear\n",
    "\n",
    "map_tokens_in_docs = {}\n",
    "dictionary_bis = {}\n",
    "special_chars = '@  www « ( ) + 0 1 2 3 4 5 6 7 8 9 / . , = - £ $ ^ %'\n",
    "\n",
    "for tok in dictionary:\n",
    "    if ((dictionary[tok].startswith(\"https://\") or dictionary[tok].startswith(\"http://\") \n",
    "         or dictionary[tok].startswith('@') or dictionary[tok].startswith('www')) is False):\n",
    "        dictionary_bis[tok] = dictionary[tok]\n",
    "\n",
    "for tok in dictionary_bis:\n",
    "    docs = []\n",
    "    for k_url, value in map_url_text.iteritems():\n",
    "        if dictionary_bis[tok] in value:\n",
    "            docs.append(k_url)\n",
    "        map_tokens_in_docs[dictionary_bis[tok]] = len(docs)\n",
    "\n",
    "# for item in map_tokens_in_docs:\n",
    "#     print map_tokens_in_docs[item]\n",
    "    \n",
    "# tfidf = term_freq * (log(total_docs/docs_with_term))\n",
    "# map token<->[url:tfidf_score]\n",
    "\n",
    "\n",
    "map_tokens_listUrlTfidf = {}\n",
    "\n",
    "for tok, list_docs in map_tokens_in_docs.iteritems():\n",
    "    list_urls_score = {}\n",
    "    for key, value in map_url_text.iteritems():\n",
    "        term_freq = value.count(tok)\n",
    "        getcontext().prec = 9\n",
    "        idf = Decimal(len(map_url_text))/Decimal(1+list_docs)\n",
    "        log_idf = math.log(idf, 2)\n",
    "        score_tfidf = Decimal(term_freq*log_idf)\n",
    "        list_urls_score[key] = score_tfidf\n",
    "    map_tokens_listUrlTfidf[tok] = list_urls_score\n",
    "\n",
    "# for t, lst in map_tokens_listUrlTfidf.iteritems():\n",
    "#     sorted_lst = sorted(lst.items(), key=operator.itemgetter(1), reverse=True)\n",
    "#     print t, sorted_lst\n",
    "\n",
    "for t, lst in map_tokens_listUrlTfidf.iteritems():\n",
    "    for url in lst:\n",
    "#         if t in url:\n",
    "        print t, url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import *\n",
    "\n",
    "\n",
    "\n",
    "def search(user_query):\n",
    "    mylist = []\n",
    "    # tokenize and stem user input\n",
    "    string = user_query.split()\n",
    "    stemmer = PorterStemmer()\n",
    "    for w in string:\n",
    "        stemmer.stem(w)\n",
    "    \n",
    "    # check if present in index\n",
    "    for i in string:\n",
    "        for t, lst in map_tokens_listUrlTfidf.iteritems():\n",
    "            if i in t:\n",
    "                for key in sorted(lst, reverse=True)[:10]:\n",
    "                    if (lst[key] != 0) or (i in key):\n",
    "                        mylist.append(key)\n",
    "                        \n",
    "    mylist = set(mylist)\n",
    "    for i in mylist:\n",
    "        print i    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.youtube.com/user/macmillancancer\n",
      "http://www.macmillan.org.uk/information-and-support/understanding-cancer/about-our-website.html\n",
      "https://my.macmillan.org.uk/Account/LogOn?returnurl=https://www.macmillan.org.uk/\n",
      "https://my.macmillan.org.uk/Profile/SavedPages\n",
      "https://www.facebook.com/macmillancancer\n",
      "http://www.pinterest.com/macmillancancer\n",
      "http://www.macmillan.org.uk/get-involved/fundraising-events/index.html\n",
      "http://www.macmillan.org.uk/get-involved/all-ways-to-help.html\n",
      "https://twitter.com/macmillancancer\n",
      "https://my.macmillan.org.uk/Account/Register?src=sso?returnurl=https://www.macmillan.org.uk/\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    search('community support')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.macmillan.org.uk/get-involved/fundraising-events/index.html\n",
      "https://my.macmillan.org.uk/Account/LogOn?returnurl=https://www.macmillan.org.uk/\n",
      "https://my.macmillan.org.uk/Account/Register?src=sso?returnurl=https://www.macmillan.org.uk/\n",
      "http://www.macmillan.org.uk/get-involved/all-ways-to-help.html\n",
      "https://www.facebook.com/macmillancancer\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    search('funraising events')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.macmillan.org.uk/get-involved/all-ways-to-help.html\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    search('medical facility')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
